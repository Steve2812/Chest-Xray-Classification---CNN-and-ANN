{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Chest X-ray Classification: Pneumonia Detection\n",
        "\n",
        "This project aims to build a binary image classification model to detect Pneumonia from chest X-ray images using deep learning. The dataset contains X-ray images labeled as either ‘NORMAL’ or ‘PNEUMONIA’, with a notable class imbalance favoring the Pneumonia category. To address this, the model uses class weighting and data augmentation techniques to improve generalization and performance. Images are preprocessed, resized to a uniform shape, and fed into a Convolutional Neural Network (CNN) with multiple layers including dropout, batch normalization, and early stopping to prevent overfitting. The training pipeline is optimized using TensorFlow’s tf.data API and supports distributed strategies like TPUs for scalable training. The final model is evaluated using accuracy and AUC metrics to assess its ability to distinguish between healthy and pneumonia-affected lungs."
      ],
      "metadata": {
        "id": "yDd-EeIsoPEu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cPBdTG0HQ4L9",
        "outputId": "337b14dd-8fa8-4238-b83f-59e3e6d99340"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section sets up the environment and prepares for distributed training. It imports essential libraries for data handling, model training, and visualization. The code attempts to detect a TPU (Tensor Processing Unit) for accelerated training and falls back to the default strategy (GPU or CPU) if a TPU is not available. This ensures hardware compatibility and performance optimization across different environments. It also prints the number of training replicas and the TensorFlow version to verify setup details."
      ],
      "metadata": {
        "id": "7GCDq76qk2CV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot\n",
        "\n",
        "\n",
        "try:\n",
        "    tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "    print('Device:', tpu.master())\n",
        "    tf.config.experimental.connect_to_cluster()\n",
        "    tf.tpu.experimental.initialize_tpu_system()\n",
        "    strategy = tf.distribute.experimental.TPUStrategy()\n",
        "\n",
        "except:\n",
        "    strategy = tf.distribute.get_strategy()\n",
        "\n",
        "print(\"number of replicas:\", strategy.num_replicas_in_sync)\n",
        "\n",
        "print(tf.__version__)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pme3twyKTqqx",
        "outputId": "8ea0f28d-de36-4c21-a8a3-3eaec2192177"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of replicas: 1\n",
            "2.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section defines key constants for the training pipeline. AUTOTUNE allows TensorFlow to optimize data loading performance automatically. EPOCHS sets the number of training iterations. GCS_PATH specifies the location of the Chest X-ray dataset in Google Drive. BATCH_SIZE is dynamically scaled based on the number of replicas in sync (TPUs or GPUs) to ensure efficient distributed training. IMAGE_SIZE sets a uniform target size for all input images, which is required for feeding them into the model."
      ],
      "metadata": {
        "id": "_MdnePNzk-Ew"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "EPOCHS = 25\n",
        "GCS_PATH = '/content/drive/MyDrive/Colab Notebooks/chest_xray/'\n",
        "BATCH_SIZE = 16 * strategy.num_replicas_in_sync\n",
        "IMAGE_SIZE = (160, 160)"
      ],
      "metadata": {
        "id": "Ynz-H85EYzU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This block loads the Chest X-ray image data from the specified training, testing, and validation directories using TensorFlow’s image_dataset_from_directory utility. Each dataset is created with inferred labels based on subdirectory names, shuffled for randomness, and resized to a uniform shape (IMAGE_SIZE). The images are loaded in RGB format with a fixed batch size of 32 and consistent seeding for reproducibility. This creates structured tf.data.Dataset objects ready for preprocessing and model training."
      ],
      "metadata": {
        "id": "GpSnzagclNkB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import image_dataset_from_directory\n",
        "train_ds_raw=image_dataset_from_directory('/content/drive/MyDrive/Colab Notebooks/chest_xray/train', labels=\"inferred\", shuffle=True, batch_size = 32, label_mode='int', seed=123, image_size=IMAGE_SIZE, color_mode='rgb')\n",
        "test_ds_raw= image_dataset_from_directory('/content/drive/MyDrive/Colab Notebooks/chest_xray/test',labels=\"inferred\", shuffle=True, batch_size = 32, label_mode='int', seed=123, image_size=IMAGE_SIZE,color_mode='rgb')\n",
        "val_ds_raw= image_dataset_from_directory('/content/drive/MyDrive/Colab Notebooks/chest_xray/val',labels=\"inferred\", shuffle=True, batch_size = 32, label_mode='int', seed=123, image_size=IMAGE_SIZE, color_mode='rgb')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3JzvASlPTPyR",
        "outputId": "b812a3ae-d247-4c21-fb31-a4b03f0e2406"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 5216 files belonging to 2 classes.\n",
            "Found 624 files belonging to 2 classes.\n",
            "Found 16 files belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After initially loading the data using image_dataset_from_directory, I noticed that the validation folder contained only 16 images, which is insufficient for reliable evaluation. To address this, I merged the training and validation image file paths using tf.io.gfile.glob, then performed a custom 80/20 split with train_test_split to create more balanced training and validation sets. This ensures a more stable validation process during model training."
      ],
      "metadata": {
        "id": "0gQJTRZLlSu5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filenames = tf.io.gfile.glob(str(GCS_PATH + '/train/*/*'))\n",
        "filenames.extend(tf.io.gfile.glob(str(GCS_PATH + '/val/*/*')))\n",
        "\n",
        "training_filenames, validation_filenames = train_test_split(filenames, test_size = 0.2)"
      ],
      "metadata": {
        "id": "OBO70vSRUE_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This block counts the number of Normal and Pneumonia images in the newly created training set by checking for the respective class names in the file paths. It uses list comprehensions to filter filenames that contain either “NORMAL” or “PNEUMONIA” and prints the count for each class. This helps verify the class distribution after the custom split and ensures that any imbalance is identified early in the process."
      ],
      "metadata": {
        "id": "P_VRnyEml8Hc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "COUNT_NORMAL = len([filename for filename in training_filenames if \"NORMAL\" in filename])\n",
        "print(\"Normal images count in training set: \" + str(COUNT_NORMAL))\n",
        "\n",
        "COUNT_PNEUMONIA = len([filename for filename in training_filenames if \"PNEUMONIA\" in filename])\n",
        "print(\"Pneumonia images count in training set: \" + str(COUNT_PNEUMONIA))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7vgSkhIrVhfz",
        "outputId": "69a47077-591a-4452-f63c-394cd16babd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normal images count in training set: 1057\n",
            "Pneumonia images count in training set: 3131\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This block defines the image preprocessing pipeline. The decode_img function reads an image file, attempts to decode it as a JPEG, converts it to a float tensor, and resizes it to the defined IMAGE_SIZE. If decoding fails (e.g., due to corruption), it returns a zero-filled dummy image to avoid breaking the pipeline. The get_label function extracts the class label from the file path by checking if the parent folder is “PNEUMONIA” and returns a binary label (1 for Pneumonia, 0 for Normal). The process_path function combines these steps to return a tuple of image and label, making it suitable for use in a tf.data pipeline. (Note: there’s a typo in 'PNUEMONIA' — it should be 'PNEUMONIA'.)"
      ],
      "metadata": {
        "id": "2QJnCUULmGJW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_img(img_path):\n",
        "    img = tf.io.read_file(img_path)\n",
        "    try:\n",
        "        img = tf.image.decode_jpeg(img, channels=3)\n",
        "    except:\n",
        "        # Return a dummy image filled with zeros if decoding fails\n",
        "        img = tf.zeros([*IMAGE_SIZE, 3], dtype=tf.float32)\n",
        "        return img\n",
        "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "    return tf.image.resize(img, IMAGE_SIZE)\n",
        "\n",
        "def get_label(file_path):\n",
        "  parts = tf.strings.split(file_path, '/')\n",
        "  label_str = parts[-2]\n",
        "  return tf.cast(label_str=='PNUEMONIA', tf.int32)\n",
        "\n",
        "def process_path(file_path):\n",
        "  label = get_label(file_path)\n",
        "  img = decode_img(file_path)\n",
        "  return img, label\n"
      ],
      "metadata": {
        "id": "x-lw2JdBf8tr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section creates TensorFlow datasets for training and validation using the previously prepared file paths. Each dataset is built by slicing the filenames into a tf.data.Dataset, then mapping each file path through the process_path function to load and preprocess the image-label pairs. The datasets are then batched according to the computed BATCH_SIZE and prefetched using AUTOTUNE to optimize performance by overlapping data loading with model execution. This setup ensures efficient, scalable input pipelines for training deep learning models."
      ],
      "metadata": {
        "id": "AsDN0lD4mY-z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = tf.data.Dataset.from_tensor_slices(training_filenames).map(process_path, num_parallel_calls = AUTOTUNE)\n",
        "val_ds = tf.data.Dataset.from_tensor_slices(validation_filenames).map(process_path, num_parallel_calls = AUTOTUNE)\n",
        "\n",
        "train_ds = train_ds.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
        "val_ds = val_ds.batch(BATCH_SIZE).prefetch(AUTOTUNE)"
      ],
      "metadata": {
        "id": "kHOI7bsLJxPC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This block calculates and prints both the number of image batches and the total number of individual images in the training and validation datasets. It uses tf.data.experimental.cardinality() to determine the number of batches, and unbatch() to flatten the datasets and count individual images. This helps verify that batching was applied correctly and ensures the expected number of images are present in each set before training begins."
      ],
      "metadata": {
        "id": "XU6gr9nwmf9g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_IMAGE_BATCHES = tf.data.experimental.cardinality(train_ds).numpy()\n",
        "VALIDATION_IMAGES_BATCHES = tf.data.experimental.cardinality(val_ds).numpy()\n",
        "\n",
        "print(TRAIN_IMAGE_BATCHES, VALIDATION_IMAGES_BATCHES)\n",
        "\n",
        "TRAIN_IMAGES = tf.data.experimental.cardinality(train_ds.unbatch()).numpy()\n",
        "VALIDATION_IMAGES = tf.data.experimental.cardinality(val_ds.unbatch()).numpy()\n",
        "\n",
        "print(TRAIN_IMAGES, VALIDATION_IMAGES)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vgqS85nOVzFz",
        "outputId": "f4804fbe-511b-4069-cda8-05532ac21be4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "262 66\n",
            "-2 -2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This block calculates class weights to address class imbalance between Normal and Pneumonia images in the training set. It computes the total number of training samples, then derives weights inversely proportional to class frequencies using a standard formula. These weights are stored in the class_weight dictionary, which will later be passed to the model during training to give more importance to the minority class and help the model learn balanced representations. The computed weights are also printed for reference."
      ],
      "metadata": {
        "id": "BSyJrxdLm7N0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total = COUNT_NORMAL + COUNT_PNEUMONIA\n",
        "weight_for_0 = total / (2 * COUNT_NORMAL)\n",
        "weight_for_1 = total / (2 * COUNT_PNEUMONIA)\n",
        "\n",
        "class_weight = {0: weight_for_0, 1: weight_for_1}\n",
        "\n",
        "print('Weight for class 0: {:.2f}'.format(weight_for_0))\n",
        "print('Weight for class 1: {:.2f}'.format(weight_for_1))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nwqvb_oC2cJm",
        "outputId": "69817fbe-c4e6-4f6a-b493-a0c452724b51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weight for class 0: 1.98\n",
            "Weight for class 1: 0.67\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This block defines the CNN model architecture for binary classification of chest X-ray images. It uses a Sequential model with an initial rescaling layer followed by data augmentation layers like RandomZoom, RandomShear, and RandomFlip to improve generalization. The model includes two convolutional blocks with ReLU activations, batch normalization, dropout for regularization, and max pooling for downsampling. After flattening, the output passes through three dense layers with dropout and batch normalization before reaching the final output layer with a sigmoid activation for binary classification. The model is compiled using the Adam optimizer, binary cross-entropy loss, and tracks both accuracy and AUC as performance metrics. An EarlyStopping callback is set to prevent overfitting by restoring the best weights when validation loss stops improving."
      ],
      "metadata": {
        "id": "zdL2pWdWnZ7q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import RandomShear, Rescaling, RandomZoom, Dense, Conv2D, MaxPooling2D, RandomFlip, Dropout, BatchNormalization, Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Rescaling(1./255, input_shape=(160,160,3)))\n",
        "model.add(RandomZoom(0.2))\n",
        "model.add(RandomShear())\n",
        "model.add(RandomFlip())\n",
        "\n",
        "model.add(Conv2D(filters = 64, kernel_size=3, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "model.add(MaxPooling2D())\n",
        "\n",
        "model.add(Conv2D(filters = 32, kernel_size=3, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "model.add(MaxPooling2D())\n",
        "\n",
        "\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "ES = EarlyStopping(monitor = \"val_loss\", patience = 5, restore_best_weights = True)\n",
        "\n",
        "model.compile(optimizer = 'Adam', loss = 'binary_crossentropy', metrics = ['accuracy', tf.keras.metrics.AUC()])\n",
        "\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fG5F1y6TWB-N",
        "outputId": "5f140923-315e-40e1-db19-b19d77d8810e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/preprocessing/tf_data_layer.py:19: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ rescaling (\u001b[38;5;33mRescaling\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ random_zoom (\u001b[38;5;33mRandomZoom\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ random_shear (\u001b[38;5;33mRandomShear\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ random_flip (\u001b[38;5;33mRandomFlip\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m158\u001b[0m, \u001b[38;5;34m158\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │         \u001b[38;5;34m1,792\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m158\u001b[0m, \u001b[38;5;34m158\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │           \u001b[38;5;34m256\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m158\u001b[0m, \u001b[38;5;34m158\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m79\u001b[0m, \u001b[38;5;34m79\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m77\u001b[0m, \u001b[38;5;34m77\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │        \u001b[38;5;34m18,464\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m77\u001b[0m, \u001b[38;5;34m77\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m128\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m77\u001b[0m, \u001b[38;5;34m77\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m38\u001b[0m, \u001b[38;5;34m38\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m46208\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │    \u001b[38;5;34m23,659,008\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │         \u001b[38;5;34m2,048\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m131,328\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │         \u001b[38;5;34m1,024\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m32,896\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_4           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │           \u001b[38;5;34m512\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m129\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ rescaling (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Rescaling</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ random_zoom (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">RandomZoom</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ random_shear (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">RandomShear</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ random_flip (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">RandomFlip</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">158</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">158</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,792</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">158</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">158</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">158</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">158</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">79</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">79</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">77</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">77</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,464</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">77</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">77</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">77</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">77</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">38</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">38</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">46208</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │    <span style=\"color: #00af00; text-decoration-color: #00af00\">23,659,008</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_4           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m23,847,585\u001b[0m (90.97 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23,847,585</span> (90.97 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m23,845,601\u001b[0m (90.96 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23,845,601</span> (90.96 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,984\u001b[0m (7.75 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,984</span> (7.75 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This line trains the model using the raw training and test datasets over 25 epochs with a batch size of 16. It applies the previously defined class_weight dictionary to give higher importance to the minority class (Pneumonia) during training, helping to address class imbalance. The EarlyStopping callback monitors validation loss and stops training if it doesn’t improve for 5 consecutive epochs, restoring the best model weights. The validation data is passed to evaluate performance during training, and training metrics are stored in the history object for later visualization."
      ],
      "metadata": {
        "id": "oq9Z1Xv1npeE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(train_ds_raw, validation_data = test_ds_raw, batch_size = 16, epochs=25, callbacks = [ES], class_weight=class_weight)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-aEDKCiUWGTH",
        "outputId": "2f31f709-397a-4b66-a6e5-edd0fe33692d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m274s\u001b[0m 2s/step - accuracy: 0.7906 - auc: 0.8998 - loss: 0.4163 - val_accuracy: 0.6250 - val_auc: 0.5000 - val_loss: 11.9656\n",
            "Epoch 2/25\n",
            "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 290ms/step - accuracy: 0.9063 - auc: 0.9659 - loss: 0.2449 - val_accuracy: 0.6250 - val_auc: 0.5000 - val_loss: 24.2266\n",
            "Epoch 3/25\n",
            "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 293ms/step - accuracy: 0.9203 - auc: 0.9727 - loss: 0.2152 - val_accuracy: 0.6234 - val_auc: 0.5283 - val_loss: 6.7582\n",
            "Epoch 4/25\n",
            "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 289ms/step - accuracy: 0.9326 - auc: 0.9793 - loss: 0.1840 - val_accuracy: 0.7676 - val_auc: 0.8540 - val_loss: 0.5507\n",
            "Epoch 5/25\n",
            "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 287ms/step - accuracy: 0.9443 - auc: 0.9853 - loss: 0.1520 - val_accuracy: 0.7163 - val_auc: 0.8798 - val_loss: 1.0173\n",
            "Epoch 6/25\n",
            "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 296ms/step - accuracy: 0.9314 - auc: 0.9812 - loss: 0.1765 - val_accuracy: 0.8494 - val_auc: 0.8986 - val_loss: 0.4799\n",
            "Epoch 7/25\n",
            "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 282ms/step - accuracy: 0.9495 - auc: 0.9867 - loss: 0.1405 - val_accuracy: 0.4391 - val_auc: 0.7005 - val_loss: 3.5555\n",
            "Epoch 8/25\n",
            "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 290ms/step - accuracy: 0.9539 - auc: 0.9877 - loss: 0.1334 - val_accuracy: 0.6250 - val_auc: 0.5299 - val_loss: 3.7121\n",
            "Epoch 9/25\n",
            "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 294ms/step - accuracy: 0.9545 - auc: 0.9891 - loss: 0.1275 - val_accuracy: 0.6346 - val_auc: 0.8370 - val_loss: 1.4413\n",
            "Epoch 10/25\n",
            "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 293ms/step - accuracy: 0.9525 - auc: 0.9899 - loss: 0.1258 - val_accuracy: 0.8269 - val_auc: 0.9048 - val_loss: 0.4344\n",
            "Epoch 11/25\n",
            "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 292ms/step - accuracy: 0.9504 - auc: 0.9880 - loss: 0.1385 - val_accuracy: 0.8157 - val_auc: 0.8894 - val_loss: 0.5762\n",
            "Epoch 12/25\n",
            "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 283ms/step - accuracy: 0.9581 - auc: 0.9886 - loss: 0.1229 - val_accuracy: 0.8253 - val_auc: 0.8997 - val_loss: 0.5221\n",
            "Epoch 13/25\n",
            "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 300ms/step - accuracy: 0.9535 - auc: 0.9919 - loss: 0.1141 - val_accuracy: 0.6955 - val_auc: 0.8580 - val_loss: 1.3625\n",
            "Epoch 14/25\n",
            "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 300ms/step - accuracy: 0.9500 - auc: 0.9883 - loss: 0.1329 - val_accuracy: 0.8205 - val_auc: 0.8816 - val_loss: 0.5146\n",
            "Epoch 15/25\n",
            "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 291ms/step - accuracy: 0.9608 - auc: 0.9912 - loss: 0.1107 - val_accuracy: 0.5849 - val_auc: 0.8402 - val_loss: 1.6198\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This block evaluates the trained model on the test dataset using the evaluate() method, which returns the loss and specified metrics — in this case, binary cross-entropy loss, accuracy, and AUC. The results are printed to provide a final assessment of the model’s performance on unseen data, offering insight into how well the model generalizes beyond the training and validation sets."
      ],
      "metadata": {
        "id": "VTlnaSainvRW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = model.evaluate(test_ds_raw)\n",
        "print(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E17MZ3D7h98E",
        "outputId": "e714be12-49f7-4630-de66-47e539a63885"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 202ms/step - accuracy: 0.8395 - auc: 0.9229 - loss: 0.3853\n",
            "[0.4344245195388794, 0.8269230723381042, 0.9048486948013306]\n"
          ]
        }
      ]
    }
  ]
}